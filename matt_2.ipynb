{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset\"\n",
    "raw_img_dir = os.path.join(dataset_dir, \"raw-img\")\n",
    "\n",
    "if os.path.exists(raw_img_dir) and len(os.listdir(raw_img_dir)) > 0:\n",
    "    print(f\"Dataset already exists at {raw_img_dir}. Skipping download.\")\n",
    "else:\n",
    "    print(\"Dataset not found. Attempting to download...\")\n",
    "    try:\n",
    "        import kagglehub\n",
    "        dataset_path = kagglehub.dataset_download(\"alessiocorrado99/animals10\", path=dataset_dir)\n",
    "        print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "    except ImportError:\n",
    "        print(\"kagglehub not found. Please install it using 'pip install kagglehub'\")\n",
    "        print(\"Attempting manual download...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        print(f\"Please ensure kagglehub is properly configured and the dataset exists.\")\n",
    "        print(f\"You can also manually download the dataset from Kaggle to {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for class_name in os.listdir(f\"{dataset_dir}/raw-img\"):\n",
    "    for filename in os.listdir(f\"{dataset_dir}/raw-img/{class_name}\"):\n",
    "        data.append({\"filename\": f\"{dataset_dir}/raw-img/{class_name}/{filename}\", \"class\": class_name})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"{dataset_dir}/_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the annotations file\n",
    "df = pd.read_csv(f\"{dataset_dir}/_annotations.csv\")\n",
    "\n",
    "# Create train, test, valid directories\n",
    "for split in ['train', 'test', 'valid']:\n",
    "    os.makedirs(os.path.join(dataset_dir, split), exist_ok=True)\n",
    "\n",
    "# Split the data\n",
    "train_df, test_valid_df = train_test_split(df, test_size=0.3, stratify=df['class'], random_state=42)\n",
    "valid_df, test_df = train_test_split(test_valid_df, test_size=0.5, stratify=test_valid_df['class'], random_state=42)\n",
    "\n",
    "# Function to copy files and create new annotations\n",
    "def process_split(split_df, split_name):\n",
    "    new_annotations = []\n",
    "    for _, row in split_df.iterrows():\n",
    "        src = row['filename']\n",
    "        dst = os.path.join(dataset_dir, split_name, os.path.basename(src))\n",
    "        shutil.copy(src, dst)\n",
    "        new_annotations.append({'filename': os.path.basename(src), 'class': row['class']})\n",
    "    \n",
    "    new_df = pd.DataFrame(new_annotations)\n",
    "    new_df.to_csv(os.path.join(dataset_dir, f'{split_name}_annotations.csv'), index=False)\n",
    "\n",
    "# Process each split\n",
    "process_split(train_df, 'train')\n",
    "process_split(valid_df, 'valid')\n",
    "process_split(test_df, 'test')\n",
    "\n",
    "print(\"Data split and reorganized into train, test, and valid directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18325 validated image filenames belonging to 10 classes.\n",
      "Found 3927 validated image filenames belonging to 10 classes.\n",
      "Found 18325 images belonging to 10 classes in the training set.\n",
      "Found 3927 images belonging to 10 classes in the validation set.\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "dataset_dir = \"dataset\"\n",
    "raw_img_dir = os.path.join(dataset_dir, \"raw-img\")\n",
    "annotations_file = os.path.join(dataset_dir, \"_annotations.csv\")\n",
    "\n",
    "# Create annotations file if it doesn't exist\n",
    "if not os.path.exists(annotations_file):\n",
    "    data = []\n",
    "    for class_name in os.listdir(raw_img_dir):\n",
    "        class_dir = os.path.join(raw_img_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            data.append({\"filename\": os.path.join(class_dir, filename), \"class\": class_name})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(annotations_file, index=False)\n",
    "\n",
    "# Read the annotations file\n",
    "df = pd.read_csv(annotations_file)\n",
    "\n",
    "# Create train, test, valid directories\n",
    "for split in ['train', 'test', 'valid']:\n",
    "    os.makedirs(os.path.join(dataset_dir, split), exist_ok=True)\n",
    "\n",
    "# Split the data\n",
    "train_df, test_valid_df = train_test_split(df, test_size=0.3, stratify=df['class'], random_state=42)\n",
    "valid_df, test_df = train_test_split(test_valid_df, test_size=0.5, stratify=test_valid_df['class'], random_state=42)\n",
    "\n",
    "# Function to create new annotations\n",
    "def create_split_annotations(split_df, split_name):\n",
    "    split_dir = os.path.join(dataset_dir, split_name)\n",
    "    split_df['filename'] = split_df['filename'].apply(lambda x: os.path.join(split_dir, os.path.basename(x)))\n",
    "    split_df.to_csv(os.path.join(dataset_dir, f'{split_name}_annotations.csv'), index=False)\n",
    "\n",
    "# Create split annotations\n",
    "create_split_annotations(train_df, 'train')\n",
    "create_split_annotations(valid_df, 'valid')\n",
    "create_split_annotations(test_df, 'test')\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Set up data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.read_csv(os.path.join(dataset_dir, 'train_annotations.csv')),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.read_csv(os.path.join(dataset_dir, 'valid_annotations.csv')),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(f\"Found {len(train_generator.filenames)} images belonging to {len(train_generator.class_indices)} classes in the training set.\")\n",
    "print(f\"Found {len(validation_generator.filenames)} images belonging to {len(validation_generator.class_indices)} classes in the validation set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-cls.pt to 'yolov8n-cls.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.31M/5.31M [00:00<00:00, 17.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.11.9 torch-2.5.1+cpu CPU (AMD Ryzen 9 9950X 16-Core Processor)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=d:\\Github\\MachineLearningFinalProject\\dataset_cls, epochs=30, time=None, patience=5, batch=64, imgsz=224, save=True, save_period=5, cache=False, device=cpu, workers=8, project=d:\\Github\\MachineLearningFinalProject\\yolo_checkpoints, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=d:\\Github\\MachineLearningFinalProject\\yolo_checkpoints\\train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\Github\\MachineLearningFinalProject\\dataset_cls\\train... found 14950 images in 9 classes: ERROR  requires 10 classes, not 9\n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\Github\\MachineLearningFinalProject\\dataset_cls\\val... found 3204 images in 9 classes: ERROR  requires 10 classes, not 9\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\Github\\MachineLearningFinalProject\\dataset_cls\\test... found 3204 images in 9 classes: ERROR  requires 10 classes, not 9\n",
      "Overriding model.yaml nc=1000 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    343050  ultralytics.nn.modules.head.Classify         [256, 10]                     \n",
      "YOLOv8n-cls summary: 99 layers, 1,451,098 parameters, 1,451,098 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir d:\\Github\\MachineLearningFinalProject\\yolo_checkpoints\\train', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Github\\MachineLearningFinalProject\\dataset_cls\\train... 14950 images, 0 corrupt: 100%|██████████| 14950/14950 [00:05<00:00, 2655.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Github\\MachineLearningFinalProject\\dataset_cls\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Github\\MachineLearningFinalProject\\dataset_cls\\val... 3204 images, 0 corrupt: 100%|██████████| 3204/3204 [00:01<00:00, 2656.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Github\\MachineLearningFinalProject\\dataset_cls\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1md:\\Github\\MachineLearningFinalProject\\yolo_checkpoints\\train\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30         0G      1.245         38        224: 100%|██████████| 234/234 [01:46<00:00,  2.20it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 26/26 [00:09<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.903      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         0G     0.3363         38        224: 100%|██████████| 234/234 [01:46<00:00,  2.20it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 26/26 [00:08<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.927      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30         0G      0.263         38        224: 100%|██████████| 234/234 [01:37<00:00,  2.39it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 26/26 [00:08<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.941      0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30         0G     0.2381         38        224: 100%|██████████| 234/234 [01:38<00:00,  2.38it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 26/26 [00:08<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.941      0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30         0G     0.1936         64        224:  15%|█▍        | 35/234 [00:14<01:23,  2.40it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "# Get absolute path to current working directory\n",
    "current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Create dataset structure for classification\n",
    "dataset_dir = os.path.join(current_dir, 'dataset_cls')\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Create train/val/test directories\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(dataset_dir, split), exist_ok=True)\n",
    "\n",
    "# Load annotations\n",
    "df = pd.read_csv('dataset/_annotations.csv')\n",
    "\n",
    "# Split data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['class'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['class'], random_state=42)\n",
    "\n",
    "# Function to copy images to classification structure\n",
    "def copy_images(split_df, split_name):\n",
    "    split_dir = os.path.join(dataset_dir, split_name)\n",
    "    for _, row in split_df.iterrows():\n",
    "        class_dir = os.path.join(split_dir, row['class'])\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        \n",
    "        src = os.path.join(current_dir, row['filename'])\n",
    "        dst = os.path.join(class_dir, os.path.basename(row['filename']))\n",
    "        if os.path.exists(src) and not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "# Copy images to respective directories\n",
    "copy_images(train_df, 'train')\n",
    "copy_images(val_df, 'val')\n",
    "copy_images(test_df, 'test')\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = os.path.join(current_dir, 'yolo_checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model\n",
    "model = YOLO('yolov8n-cls.pt')  # Using YOLOv8 classification model\n",
    "\n",
    "# Train with simplified parameters\n",
    "results = model.train(\n",
    "    data=dataset_dir,\n",
    "    epochs=30,\n",
    "    imgsz=224,\n",
    "    batch=64,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    project=checkpoint_dir,\n",
    "    name='train',\n",
    "    exist_ok=True,\n",
    "    patience=5,  # Early stopping patience\n",
    "    save_period=5,  # Save checkpoint every 5 epochs\n",
    "    resume=False  # Start fresh training\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save(os.path.join(checkpoint_dir, 'final_model.pt'))\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
