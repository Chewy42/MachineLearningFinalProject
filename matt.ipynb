{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/alessiocorrado99/animals10?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586M/586M [00:35<00:00, 17.3MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\mfave\\.cache\\kagglehub\\datasets\\alessiocorrado99\\animals10\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "raw_img_dir = os.path.join(dataset_dir, \"raw-img\")\n",
    "\n",
    "if os.path.exists(raw_img_dir) and len(os.listdir(raw_img_dir)) > 0:\n",
    "    print(f\"Dataset already exists at {raw_img_dir}. Skipping download.\")\n",
    "else:\n",
    "    print(\"Dataset not found. Attempting to download...\")\n",
    "    try:\n",
    "        import kagglehub\n",
    "        dataset_path = kagglehub.dataset_download(\"alessiocorrado99/animals10\", path=dataset_dir)\n",
    "        print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "    except ImportError:\n",
    "        print(\"kagglehub not found. Please install it using 'pip install kagglehub'\")\n",
    "        print(\"Attempting manual download...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        print(f\"Please ensure kagglehub is properly configured and the dataset exists.\")\n",
    "        print(f\"You can also manually download the dataset from Kaggle to {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for class_name in os.listdir(f\"{dataset_dir}/raw-img\"):\n",
    "    for filename in os.listdir(f\"{dataset_dir}/raw-img/{class_name}\"):\n",
    "        data.append({\"filename\": f\"{dataset_dir}/raw-img/{class_name}/{filename}\", \"class\": class_name})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"{dataset_dir}/_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split and reorganized into train, test, and valid directories.\n"
     ]
    }
   ],
   "source": [
    "# Read the annotations file\n",
    "df = pd.read_csv(f\"{dataset_dir}/_annotations.csv\")\n",
    "\n",
    "# Create train, test, valid directories\n",
    "for split in ['train', 'test', 'valid']:\n",
    "    os.makedirs(os.path.join(dataset_dir, split), exist_ok=True)\n",
    "\n",
    "# Split the data\n",
    "train_df, test_valid_df = train_test_split(df, test_size=0.3, stratify=df['class'], random_state=42)\n",
    "valid_df, test_df = train_test_split(test_valid_df, test_size=0.5, stratify=test_valid_df['class'], random_state=42)\n",
    "\n",
    "# Function to copy files and create new annotations\n",
    "def process_split(split_df, split_name):\n",
    "    new_annotations = []\n",
    "    for _, row in split_df.iterrows():\n",
    "        src = row['filename']\n",
    "        dst = os.path.join(dataset_dir, split_name, os.path.basename(src))\n",
    "        shutil.copy(src, dst)\n",
    "        new_annotations.append({'filename': os.path.basename(src), 'class': row['class']})\n",
    "    \n",
    "    new_df = pd.DataFrame(new_annotations)\n",
    "    new_df.to_csv(os.path.join(dataset_dir, f'{split_name}_annotations.csv'), index=False)\n",
    "\n",
    "# Process each split\n",
    "process_split(train_df, 'train')\n",
    "process_split(valid_df, 'valid')\n",
    "process_split(test_df, 'test')\n",
    "\n",
    "print(\"Data split and reorganized into train, test, and valid directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18325 validated image filenames belonging to 10 classes.\n",
      "Found 3927 validated image filenames belonging to 10 classes.\n",
      "Found 18325 images belonging to 10 classes in the training set.\n",
      "Found 3927 images belonging to 10 classes in the validation set.\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "dataset_dir = \"dataset\"\n",
    "raw_img_dir = os.path.join(dataset_dir, \"raw-img\")\n",
    "annotations_file = os.path.join(dataset_dir, \"_annotations.csv\")\n",
    "\n",
    "# Create annotations file if it doesn't exist\n",
    "if not os.path.exists(annotations_file):\n",
    "    data = []\n",
    "    for class_name in os.listdir(raw_img_dir):\n",
    "        class_dir = os.path.join(raw_img_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            data.append({\"filename\": os.path.join(class_dir, filename), \"class\": class_name})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(annotations_file, index=False)\n",
    "\n",
    "# Read the annotations file\n",
    "df = pd.read_csv(annotations_file)\n",
    "\n",
    "# Create train, test, valid directories\n",
    "for split in ['train', 'test', 'valid']:\n",
    "    os.makedirs(os.path.join(dataset_dir, split), exist_ok=True)\n",
    "\n",
    "# Split the data\n",
    "train_df, test_valid_df = train_test_split(df, test_size=0.3, stratify=df['class'], random_state=42)\n",
    "valid_df, test_df = train_test_split(test_valid_df, test_size=0.5, stratify=test_valid_df['class'], random_state=42)\n",
    "\n",
    "# Function to create new annotations\n",
    "def create_split_annotations(split_df, split_name):\n",
    "    split_dir = os.path.join(dataset_dir, split_name)\n",
    "    split_df['filename'] = split_df['filename'].apply(lambda x: os.path.join(split_dir, os.path.basename(x)))\n",
    "    split_df.to_csv(os.path.join(dataset_dir, f'{split_name}_annotations.csv'), index=False)\n",
    "\n",
    "# Create split annotations\n",
    "create_split_annotations(train_df, 'train')\n",
    "create_split_annotations(valid_df, 'valid')\n",
    "create_split_annotations(test_df, 'test')\n",
    "\n",
    "# Set up data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.read_csv(os.path.join(dataset_dir, 'train_annotations.csv')),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.read_csv(os.path.join(dataset_dir, 'valid_annotations.csv')),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(f\"Found {len(train_generator.filenames)} images belonging to {len(train_generator.class_indices)} classes in the training set.\")\n",
    "print(f\"Found {len(validation_generator.filenames)} images belonging to {len(validation_generator.class_indices)} classes in the validation set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "286/286 [==============================] - 145s 504ms/step - loss: 2.2176 - accuracy: 0.1891 - val_loss: 2.1656 - val_accuracy: 0.2090\n",
      "Epoch 2/50\n",
      "286/286 [==============================] - 126s 440ms/step - loss: 2.1522 - accuracy: 0.2182 - val_loss: 2.1137 - val_accuracy: 0.2541\n",
      "Epoch 3/50\n",
      "286/286 [==============================] - 124s 432ms/step - loss: 2.1056 - accuracy: 0.2367 - val_loss: 2.0436 - val_accuracy: 0.2736\n",
      "Epoch 4/50\n",
      "286/286 [==============================] - 123s 430ms/step - loss: 2.0684 - accuracy: 0.2514 - val_loss: 2.0271 - val_accuracy: 0.2802\n",
      "Epoch 5/50\n",
      "286/286 [==============================] - 122s 427ms/step - loss: 2.0228 - accuracy: 0.2677 - val_loss: 1.9753 - val_accuracy: 0.2935\n",
      "Epoch 6/50\n",
      "286/286 [==============================] - 122s 427ms/step - loss: 1.9973 - accuracy: 0.2827 - val_loss: 1.9335 - val_accuracy: 0.3309\n",
      "Epoch 7/50\n",
      "286/286 [==============================] - 123s 429ms/step - loss: 1.9575 - accuracy: 0.3006 - val_loss: 1.9111 - val_accuracy: 0.3253\n",
      "Epoch 8/50\n",
      "286/286 [==============================] - 122s 425ms/step - loss: 1.9361 - accuracy: 0.3124 - val_loss: 1.8611 - val_accuracy: 0.3453\n",
      "Epoch 9/50\n",
      "286/286 [==============================] - 122s 426ms/step - loss: 1.9005 - accuracy: 0.3218 - val_loss: 1.8656 - val_accuracy: 0.3530\n",
      "Epoch 10/50\n",
      "286/286 [==============================] - 121s 424ms/step - loss: 1.8817 - accuracy: 0.3361 - val_loss: 1.8579 - val_accuracy: 0.3540\n",
      "Epoch 11/50\n",
      "286/286 [==============================] - 121s 424ms/step - loss: 1.8548 - accuracy: 0.3524 - val_loss: 1.7794 - val_accuracy: 0.3801\n",
      "Epoch 12/50\n",
      "286/286 [==============================] - 122s 428ms/step - loss: 1.8228 - accuracy: 0.3506 - val_loss: 1.7102 - val_accuracy: 0.4073\n",
      "Epoch 13/50\n",
      "286/286 [==============================] - 123s 430ms/step - loss: 1.7843 - accuracy: 0.3708 - val_loss: 1.6984 - val_accuracy: 0.4088\n",
      "Epoch 14/50\n",
      "286/286 [==============================] - 123s 432ms/step - loss: 1.7810 - accuracy: 0.3694 - val_loss: 1.6991 - val_accuracy: 0.3950\n",
      "Epoch 15/50\n",
      "286/286 [==============================] - 124s 432ms/step - loss: 1.7406 - accuracy: 0.3891 - val_loss: 1.6663 - val_accuracy: 0.4226\n",
      "Epoch 16/50\n",
      "286/286 [==============================] - 127s 443ms/step - loss: 1.7438 - accuracy: 0.3936 - val_loss: 1.6773 - val_accuracy: 0.4032\n",
      "Epoch 17/50\n",
      "286/286 [==============================] - 123s 430ms/step - loss: 1.7167 - accuracy: 0.3965 - val_loss: 1.6058 - val_accuracy: 0.4298\n",
      "Epoch 18/50\n",
      "286/286 [==============================] - 121s 424ms/step - loss: 1.6880 - accuracy: 0.4151 - val_loss: 1.6061 - val_accuracy: 0.4462\n",
      "Epoch 19/50\n",
      "286/286 [==============================] - 122s 427ms/step - loss: 1.6888 - accuracy: 0.4158 - val_loss: 1.5818 - val_accuracy: 0.4467\n",
      "Epoch 20/50\n",
      "207/286 [====================>.........] - ETA: 31s - loss: 1.6493 - accuracy: 0.4366"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Assuming input_shape and num_classes are defined\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "model = create_cnn_model(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the model\n",
    "model.save('animal_classification_model.tf')\n",
    "print(\"Model saved as animal_classification_model.tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('animal_classification_model.tf')\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    # Load and resize image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    # Convert to array and add batch dimension\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # Normalize pixel values\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array, img\n",
    "\n",
    "# Function to predict and display results\n",
    "def predict_and_display(image_path):\n",
    "    # Get class names from your training data\n",
    "    class_names = list(train_generator.class_indices.keys())\n",
    "    \n",
    "    # Preprocess the image\n",
    "    img_array, original_img = preprocess_image(image_path)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = np.max(predictions[0]) * 100\n",
    "    \n",
    "    # Display image and results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(original_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Predicted: {predicted_class}\\nConfidence: {confidence:.2f}%')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top 3 predictions\n",
    "    top_3_idx = np.argsort(predictions[0])[-3:][::-1]\n",
    "    print(\"\\nTop 3 Predictions:\")\n",
    "    for idx in top_3_idx:\n",
    "        print(f\"{class_names[idx]}: {predictions[0][idx]*100:.2f}%\")\n",
    "\n",
    "# Example usage - replace with path to your test image\n",
    "test_image_path = \"dataset/test/your_test_image.jpg\"  # Replace with actual path\n",
    "predict_and_display(test_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
