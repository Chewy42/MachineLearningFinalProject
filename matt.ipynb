{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at dataset\\raw-img. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "raw_img_dir = os.path.join(dataset_dir, \"raw-img\")\n",
    "\n",
    "if os.path.exists(raw_img_dir) and len(os.listdir(raw_img_dir)) > 0:\n",
    "    print(f\"Dataset already exists at {raw_img_dir}. Skipping download.\")\n",
    "else:\n",
    "    print(\"Dataset not found. Attempting to download...\")\n",
    "    try:\n",
    "        import kagglehub\n",
    "        dataset_path = kagglehub.dataset_download(\"alessiocorrado99/animals10\", path=dataset_dir)\n",
    "        print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "    except ImportError:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18325 validated image filenames belonging to 10 classes.\n",
      "Found 3927 validated image filenames belonging to 10 classes.\n",
      "Found 18325 images belonging to 10 classes in the training set.\n",
      "Found 3927 images belonging to 10 classes in the validation set.\n"
     ]
    }
   ],
   "source": [
    "def generate_annotations():\n",
    "    data = []\n",
    "    for class_name in os.listdir(f\"{dataset_dir}/raw-img\"):\n",
    "        for filename in os.listdir(f\"{dataset_dir}/raw-img/{class_name}\"):\n",
    "            data.append({\"filename\": f\"{dataset_dir}/raw-img/{class_name}/{filename}\", \"class\": class_name})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f\"{dataset_dir}/_annotations.csv\", index=False)\n",
    "\n",
    "generate_annotations()\n",
    "\n",
    "def create_dataset_split(dataset_dir, use_full_paths=True):\n",
    "    df = pd.read_csv(f\"{dataset_dir}/_annotations.csv\")\n",
    "    \n",
    "    for split in ['train', 'test', 'valid']:\n",
    "        os.makedirs(os.path.join(dataset_dir, split), exist_ok=True)\n",
    "    \n",
    "    train_df, test_valid_df = train_test_split(df, test_size=0.3, stratify=df['class'], random_state=42)\n",
    "    valid_df, test_df = train_test_split(test_valid_df, test_size=0.5, stratify=test_valid_df['class'], random_state=42)\n",
    "    \n",
    "    def process_split(split_df, split_name):\n",
    "        split_dir = os.path.join(dataset_dir, split_name)\n",
    "        new_annotations = []\n",
    "        \n",
    "        for _, row in split_df.iterrows():\n",
    "            src = row['filename']\n",
    "            dst = os.path.join(split_dir, os.path.basename(src))\n",
    "            shutil.copy(src, dst)\n",
    "            \n",
    "            filename = dst if use_full_paths else os.path.basename(src)\n",
    "            new_annotations.append({'filename': filename, 'class': row['class']})\n",
    "        \n",
    "        new_df = pd.DataFrame(new_annotations)\n",
    "        new_df.to_csv(os.path.join(dataset_dir, f'{split_name}_annotations.csv'), index=False)\n",
    "        return new_df\n",
    "    \n",
    "    train_df = process_split(train_df, 'train')\n",
    "    valid_df = process_split(valid_df, 'valid')\n",
    "    test_df = process_split(test_df, 'test')\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "train_df, valid_df, test_df = create_dataset_split(dataset_dir, use_full_paths=True)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.read_csv(os.path.join(dataset_dir, 'train_annotations.csv')),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.read_csv(os.path.join(dataset_dir, 'valid_annotations.csv')),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Found {len(train_generator.filenames)} images belonging to {len(train_generator.class_indices)} classes in the training set.\")\n",
    "print(f\"Found {len(validation_generator.filenames)} images belonging to {len(validation_generator.class_indices)} classes in the validation set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "base_model = applications.ResNet50V2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/286 [..............................] - ETA: 4:34 - loss: 3.2551 - accuracy: 0.0859 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m base_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "base_model = model.layers[0]\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "model.save('animal_classification_model.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on 25 Test Images into the Inference Examples Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading weights for layer: batch_normalization\n",
      "Loading batch_normalization/batch_normalization/gamma:0\n",
      "Weight shape in file: (2048,)\n",
      "Layer weight shape: (2048,)\n",
      "Loading batch_normalization/batch_normalization/beta:0\n",
      "Weight shape in file: (2048,)\n",
      "Layer weight shape: (2048,)\n",
      "Loading batch_normalization/batch_normalization/moving_mean:0\n",
      "Weight shape in file: (2048,)\n",
      "Layer weight shape: (2048,)\n",
      "Loading batch_normalization/batch_normalization/moving_variance:0\n",
      "Weight shape in file: (2048,)\n",
      "Layer weight shape: (2048,)\n",
      "\n",
      "Loading weights for layer: dense_4\n",
      "Loading dense_4/dense_4/kernel:0\n",
      "Weight shape in file: (2048, 512)\n",
      "Layer weight shape: (2048, 512)\n",
      "Loading dense_4/dense_4/bias:0\n",
      "Weight shape in file: (512,)\n",
      "Layer weight shape: (512,)\n",
      "\n",
      "Loading weights for layer: dense_5\n",
      "Loading dense_5/dense_5/kernel:0\n",
      "Weight shape in file: (512, 256)\n",
      "Layer weight shape: (512, 256)\n",
      "Loading dense_5/dense_5/bias:0\n",
      "Weight shape in file: (256,)\n",
      "Layer weight shape: (256,)\n",
      "\n",
      "Loading weights for layer: dense_6\n",
      "Loading dense_6/dense_6/kernel:0\n",
      "Weight shape in file: (256, 10)\n",
      "Layer weight shape: (256, 10)\n",
      "Loading dense_6/dense_6/bias:0\n",
      "Weight shape in file: (10,)\n",
      "Layer weight shape: (10,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_folder, random_image)\n\u001b[0;32m     83\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_examples/classification_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 84\u001b[0m \u001b[43mpredict_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 55\u001b[0m, in \u001b[0;36mpredict_and_save\u001b[1;34m(image_path, class_names, save_path)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_and_save\u001b[39m(image_path, class_names, save_path):\n\u001b[1;32m---> 55\u001b[0m     img_array, original_img \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img_array)\n\u001b[0;32m     57\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m class_names[np\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m])]\n",
      "Cell \u001b[1;32mIn[19], line 50\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m img \u001b[38;5;241m=\u001b[39m load_img(image_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m     49\u001b[0m img_array \u001b[38;5;241m=\u001b[39m img_to_array(img)\n\u001b[1;32m---> 50\u001b[0m img_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m img_array \u001b[38;5;241m=\u001b[39m img_array \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_array, img\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "base_model = applications.ResNet50V2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(name='global_average_pooling2d'),\n",
    "    layers.BatchNormalization(name='batch_normalization'),\n",
    "    layers.Dense(512, activation='relu', name='dense_4'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu', name='dense_5'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax', name='dense_6')\n",
    "])\n",
    "\n",
    "dummy_input = tf.random.normal((1, 224, 224, 3))\n",
    "temp = model(dummy_input)\n",
    "\n",
    "with h5py.File('checkpoints/model_checkpoint.h5', 'r') as f:\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.Model):\n",
    "            continue\n",
    "        if hasattr(layer, 'weights') and len(layer.weights) > 0:\n",
    "            print(f\"\\nLoading weights for layer: {layer.name}\")\n",
    "            for weight in layer.weights:\n",
    "                weight_name = weight.name.replace(layer.name + '/', '')\n",
    "                weight_path = f\"{layer.name}/{layer.name}/{weight_name}\"\n",
    "                if weight_path in f:\n",
    "                    print(f\"Loading {weight_path}\")\n",
    "                    weight_value = f[weight_path][:]\n",
    "                    print(f\"Weight shape in file: {weight_value.shape}\")\n",
    "                    print(f\"Layer weight shape: {weight.shape}\")\n",
    "                    if weight_value.shape == weight.shape:\n",
    "                        weight.assign(weight_value)\n",
    "                    else:\n",
    "                        print(f\"Shape mismatch for {weight_path}\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array, img\n",
    "\n",
    "def predict_and_save(image_path, class_names, save_path):\n",
    "    img_array, original_img = preprocess_image(image_path)\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = np.max(predictions[0]) * 100\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(original_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Predicted: {predicted_class}\\nConfidence: {confidence:.2f}%')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \n",
    "    top_3_idx = np.argsort(predictions[0])[-3:][::-1]\n",
    "    print(f\"\\nResults for {os.path.basename(save_path)}:\")\n",
    "    for idx in top_3_idx:\n",
    "        print(f\"{class_names[idx]}: {predictions[0][idx]*100:.2f}%\")\n",
    "\n",
    "class_names = ['Butterfly', 'Cat', 'Chicken', 'Cow', 'Dog', \n",
    "               'Elephant', 'Horse', 'Spider', 'Goat', 'Squirrel']\n",
    "\n",
    "os.makedirs('inference_examples', exist_ok=True)\n",
    "\n",
    "test_folder = \"dataset/test\"\n",
    "test_images = [f for f in os.listdir(test_folder) if f.endswith('.jpeg')]\n",
    "\n",
    "for i in range(23):\n",
    "    random_image = random.choice(test_images)\n",
    "    test_image_path = os.path.join(test_folder, random_image)\n",
    "    save_path = f'inference_examples/classification_{i+3}.png'\n",
    "    predict_and_save(test_image_path, class_names, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
